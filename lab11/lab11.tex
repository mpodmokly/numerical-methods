\documentclass[11pt, leqno]{scrartcl}
\usepackage{polski}
\usepackage[polish]{babel}

\usepackage{graphicx, float, caption, subcaption}
\usepackage{tabularx, multirow, hyperref, enumitem}
\usepackage{listings, xcolor}
\usepackage{amsmath, amssymb}
%\usepackage{minted}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=black,
    citecolor=black
}

\definecolor{md-black}{rgb}{0.12, 0.12, 0.12}
\definecolor{md-teal}{rgb}{0.38, 0.79, 0.69}
\definecolor{md-mauve}{rgb}{0.76, 0.52, 0.75}
\definecolor{md-yellow}{rgb}{0.86, 0.86, 0.67}
\definecolor{md-green}{rgb}{0.13, 0.55, 0.13}
\definecolor{md-red}{rgb}{0.82, 0.10, 0.14}
\definecolor{md-purple}{rgb}{0.69, 0.33, 0.73}
\definecolor{md-orange}{rgb}{0.96, 0.42, 0.18}
\definecolor{md-gray}{rgb}{0.44, 0.46, 0.51}
\lstset{
    language=Python,
    basicstyle=\color{md-teal}\ttfamily,
    keywordstyle=\color{md-mauve},
    commentstyle=\color{md-green},
    stringstyle=\color{md-red},
    numbers=left,
    numberstyle=\small\color{md-gray}\ttfamily,
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{md-black},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=none,
    tabsize=4,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    escapeinside={\%*}{*)},
    numbersep=-10pt,
    morekeywords={as},
    classoffset=1,
    morekeywords={quad, quad_vec, trapz, simps, linregress,
        newton},
    keywordstyle=\color{md-yellow},
    classoffset=0
}

\graphicspath{{../images/}}

\title{Laboratorium 11 - Optymalizacja}
\author{Mateusz Podmokły - II rok Informatyka WI}
\date{6 czerwiec 2024}

\begin{document}
    \maketitle
    \section{Treść zadania}
    \textbf{Zadanie 1.} Wyznacz punkty krytyczne każdej z poniższych
    funkcji. Scharakteryzuj każdy znaleziony punkt jako minimum,
    maksimum lub punkt siodłowy. Dla każdej funkcji zbadaj, czy
    posiada minimum globalne lub maksimum globalne na zbiorze
    $\mathbb{R}^2$.
    \begin{align*}
        &f_1(x,y)=x^2-4xy+y^2 \\
        &f_2(x,y)=x^4-4xy+y^4 \\
        &f_3(x,y)=2x^3-3x^2-6xy(x-y-1) \\
        &f_4(x,y)=(x-y)^4+x^2-y^2-2x+2y+1
    \end{align*}

    \subsection*{}
    \textbf{Zadanie 2.} Funkcja celu użyta do optymalizacji
    $F(x^{(0)},x^{(1)},\dots ,x^{(n)})$ zdefiniowana jest jako
    \[
        F(x^{(0)},x^{(1)},\dots ,x^{(n)})=\lambda _1
            \sum_{i=0}^{n}\sum_{j=1}^{k}\frac{1}
            {\epsilon +||x^{(i)}-r{(j)}||_2^2}+\lambda _2
            \sum_{i=0}^{n-1}||x^{(i+1)}-x^{(i)}||_2^2
    \]

    \section{Specyfikacja użytego środowiska}
    Specyfikacja:
    \begin{itemize}
        \item Środowisko: Visual Studio Code,
        \item Język programowania: Python,
        \item System operacyjny: Microsoft Windows 11,
        \item Architektura systemu: x64.
    \end{itemize}

    \section{Rozwiązanie problemu}
    \subsection{Biblioteki}
    W realizacji rozwiązania wykorzystane zostały następujące
    biblioteki:
    \begin{lstlisting}
    import numpy as np
    \end{lstlisting}

    \subsection{Zadanie 1.}
    Punkty krytyczne zostały znalezione przy pomocny metody Newtona.
    Metoda Newtona wybiera punkt startowy $x_0$ i wykonuje kolejne
    iteracje przybliżające rozwiązanie:
    \[
        x_{k+1}=x_k-H(x_k)^{-1}\nabla f(x_k)
    \]
    gdzie $\nabla f(x,y)$ to gradient funkcji $f$ dany wzorem
    \[
        \nabla f(x,y)=
        \begin{bmatrix}
            \frac{\partial f}{\partial x}(x,y) \\
            \frac{\partial f}{\partial y}(x,y)
        \end{bmatrix}
    \]
    a $H(x,y)$ to macierz Hessego dana wzorem
    \[
        H(x,y)=
        \begin{bmatrix}
            \frac{\partial ^2f}{\partial x^2}(x,y) &
                \frac{\partial ^2f}{\partial x\partial y}(x,y) \\
            \frac{\partial ^2f}{\partial y\partial x}(x,y) &
            \frac{\partial ^2f}{\partial y^2}(x,y)
        \end{bmatrix}
    \]
    Punkt krytyczny jest minimum, jeżeli wszystkie wartości własne
    macierzy Hessego są większe od 0, maksimum jeżeli są mniejsze od
    0, a punktem siodłowym w każdej innej sytuacji. Wartości własne
    macierzy $A$ można wyznaczyć z równania
    \[
        det(A-\lambda I)=0
    \]
    gdzie $det()$ to wyznacznik macierzy, a $I$ to macierz
    jednostkowa. Do obliczenia wartości własnych wykorzystałem
    funkcję z biblioteki \texttt{NumPy}
    \[
        \texttt{np.linalg.eig}
    \]

    \subsection{Zadanie 2.}
    \subsubsection{Gradient funkcji celu}
    Na początku wyprowadzamy wzór na pochodną cząstkową funkcji
    celu $F$. Dla $i \in [1,n-1]$ mamy
    \[
        \frac{\partial F}{\partial x^{(i)}}=2\lambda _2
            (x_{i+1}-x_{i-1})-\lambda _1\sum_{j=1}^{k}
            \frac{2(x_i-r_j)}{\epsilon +||x_i-r_j||^4}
    \]
    dla $i = 0$
    \[
        \frac{\partial F}{\partial x^{(0)}}=2\lambda _2
            (x_{1}-x_{0})-\lambda _1\sum_{j=1}^{k}
            \frac{2(x_0-r_j)}{\epsilon +||x_0-r_j||^4}
    \]
    a dla $i = n$
    \[
        \frac{\partial F}{\partial x^{(n)}}=2\lambda _2
            (x_{n}-x_{n-1})-\lambda _1\sum_{j=1}^{k}
            \frac{2(x_n-r_j)}{\epsilon +||x_n-r_j||^4}
    \]
    Zatem, gradient funkcji $F$ dany jest wzorem
    \[
        \nabla F=
        \begin{bmatrix}
            \frac{\partial F}{\partial x^{(0)}} \\
            \vdots \\
            \frac{\partial F}{\partial x^{(n)}}
        \end{bmatrix}
    \]

    \subsubsection{Algorytm największego spadku}
    Algorytm iteracyjny polega na przybliżaniu minimum zadanej
    funkcji celu $F$. Polega na wybraniu punktu początkowego
    $x_0$ i obliczaniu w kolejnych iteracjach
    \[
        x_{k+1}=x_k+\alpha _kd_k
    \]
    gdzie
    \[
        d_k=-\nabla F(x_k)
    \]
    a $\alpha _k$ to minimum funkcji $F$ wzdłuż kierunku $d_k$.

    \section{Przedstawienie wyników}
    \subsection{Zadanie 1.}
    Zestawienie punktów krytycznych funkcji. \\
    Funkcja $f_1$:
    \begin{itemize}
        \item $(0,0)$ - punkt siodłowy, $x_0=(1,1)$
    \end{itemize}
    Funkcja $f_2$:
    \begin{itemize}
        \item $(-1,-1)$ - minimum, $x_0=(-2,-2)$
        \item $(1,1)$ - minimum, $x_0=(2,2)$
        \item $(0,0)$ - punkt siodłowy, $x_0=(-0.5,0)$
    \end{itemize}
    Funkcja $f_3$:
    \begin{itemize}
        \item $(-1,-1)$ - maksimum, $x_0=(-2,-2)$
        \item $(1,0)$ - minimum, $x_0=(2,0)$
        \item $(0,-1)$ - punkt siodłowy, $x_0=(0,-2)$
        \item $(0,0)$ - punkt siodłowy, $x_0=(0,1)$
    \end{itemize}
    Funkcja $f_4$:
    \begin{itemize}
        \item $(1,1)$ - punkt siodłowy, $x_0=(0,0)$
    \end{itemize}

    \section{Wnioski}
    Metoda Newtona jest skutecznym sposobem znajdowania punktów
    krytycznych funkcji. Powinniśmy pamiętać o odpowiednim doborze
    punktów początkowych $x_0$. \\
    Optymalizacja funkcji celu jest przydatnym zagadnieniem
    odpowiadającym na szeroki zakres problemów.

    \section{Bibliografia}
    \url{https://pl.wikipedia.org/wiki/Metoda_Newtona_
        (optymalizacja)} \\
    \url{https://pl.wikipedia.org/wiki/Metoda_najszybszego_spadku} \\
    \url{https://pl.wikipedia.org/wiki/Gradient_(matematyka)} \\
    \url{https://home.agh.edu.pl/~gora/algebra/Wyklad07.pdf} \\
    \url{https://pl.wikipedia.org/wiki/Macierz_Hessego} \\
    \url{https://pl.wikipedia.org/wiki/Pochodna_cz%C4%85stkowa}

\end{document}
